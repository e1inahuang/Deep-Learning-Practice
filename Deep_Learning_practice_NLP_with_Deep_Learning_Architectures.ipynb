{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D5wJAob6SaOj",
        "PJPK8S1ySeX7",
        "-zc8baugSeRT",
        "iH_CO46XVWw-",
        "6B1v2V9FVWkU",
        "-74p0vEYVWcM",
        "zSI818L0SeHi",
        "jOXpORTKSeFL",
        "Qb0hn3PGfDaV",
        "19mB7OYd7pZF",
        "qPkNcc2M7pCi",
        "F-t2aRjMSpZI",
        "BSASWI8mSwcY",
        "epeG2t4tWTBS"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1 - Sentiment Analysis using recurrent models\n"
      ],
      "metadata": {
        "id": "D5wJAob6SaOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Import"
      ],
      "metadata": {
        "id": "PJPK8S1ySeX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cz5RXcseTV9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('/content/drive/MyDrive/DS301/IMDB Dataset.csv')\n",
        "df.sentiment = (df.sentiment == \"positive\").astype(\"int\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "l0ccTSBWSeVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_df, temp_df = train_test_split(df, train_size=0.7, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, train_size=(0.15/0.3), random_state=42)\n",
        "\n",
        "# Preprocessing function to clean and tokenize text\n",
        "def process_tokens(text):\n",
        "    # Using regular expressions to remove punctuation and numbers\n",
        "    return re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower()).split()\n",
        "\n",
        "# Simplified preprocessing function\n",
        "def preprocessing(data):\n",
        "    return [process_tokens(sentence) for sentence in data]\n",
        "\n",
        "# The text and labels are in the first and second columns respectively\n",
        "train_texts, train_labels = train_df.iloc[:, 0].values, train_df.iloc[:, 1].astype('int')\n",
        "val_texts, val_labels = val_df.iloc[:, 0].values, val_df.iloc[:, 1].astype('int')\n",
        "test_texts, test_labels = test_df.iloc[:, 0].values, test_df.iloc[:, 1].astype('int')\n",
        "\n",
        "# Process texts\n",
        "train_data = preprocessing(train_texts)\n",
        "val_data = preprocessing(val_texts)\n",
        "test_data = preprocessing(test_texts)\n",
        "\n",
        "# Vectorizer using a simple Bag of Words model\n",
        "class Vectorizer:\n",
        "    def __init__(self, max_features=2000):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = []\n",
        "        self.token_to_index = {}\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        # Flatten the dataset and get the most common tokens\n",
        "        all_tokens = [token for sublist in dataset for token in sublist]\n",
        "        most_common_tokens = [token for token, _ in Counter(all_tokens).most_common(self.max_features)]\n",
        "        self.vocab_list = most_common_tokens\n",
        "        self.token_to_index = {token: idx for idx, token in enumerate(self.vocab_list)}\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        data_matrix = np.zeros((len(dataset), self.max_features))\n",
        "        for i, sentence in enumerate(dataset):\n",
        "            for token in sentence:\n",
        "                index = self.token_to_index.get(token)\n",
        "                if index is not None:\n",
        "                    data_matrix[i, index] += 1\n",
        "        return data_matrix\n",
        "\n",
        "# Initialize and fit the vectorizer\n",
        "vectorizer = Vectorizer()\n",
        "vectorizer.fit(train_data)\n",
        "\n",
        "# Transform datasets\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "# Encode labels\n",
        "y_train = to_categorical(train_labels, 2)\n",
        "y_val = to_categorical(val_labels, 2)\n",
        "y_test = to_categorical(test_labels, 2)\n",
        "\n",
        "# Reshaping the data to fit the model input shape\n",
        "X_train = X_train[:, np.newaxis, :]\n",
        "X_val = X_val[:, np.newaxis, :]\n",
        "X_test = X_test[:, np.newaxis, :]\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')\n"
      ],
      "metadata": {
        "id": "NOGpkqwslW9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. RNN"
      ],
      "metadata": {
        "id": "-zc8baugSeRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine max_features\n",
        "actual_vocab_size = len(vectorizer.vocab_list)\n",
        "print(f\"The actual vocabulary size used: {actual_vocab_size}\")"
      ],
      "metadata": {
        "id": "TrOcTUF1sClJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define and compile the RNN model in a function\n",
        "def compile_and_train_rnn(X_train, y_train, X_val, y_val, input_shape, units=256, learning_rate=0.01, epochs=10, batch_size=256):\n",
        "    # Initialize the Sequential model\n",
        "    model = Sequential([\n",
        "        SimpleRNN(units, input_shape=input_shape, activation='tanh'),\n",
        "        Dense(2, activation='softmax')  # Output layer for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with Adam optimizer and a learning rate of 0.01\n",
        "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return test_accuracy\n",
        "\n",
        "max_features = 2000\n",
        "# Train the model and print the test accuracy\n",
        "test_accuracy = compile_and_train_rnn(X_train, y_train, X_val, y_val, input_shape=(1, max_features))\n",
        "print(f'Test accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "SgV2WAbdVW4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. LSTM"
      ],
      "metadata": {
        "id": "iH_CO46XVWw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# Define and compile the LSTM model in a function\n",
        "def compile_and_train_lstm(X_train, y_train, X_val, y_val, input_shape, units=256, learning_rate=0.01, epochs=10, batch_size=256):\n",
        "    # Initialize the Sequential model\n",
        "    model = Sequential([\n",
        "        LSTM(units, input_shape=input_shape, activation='tanh'),\n",
        "        Dense(2, activation='softmax')  # Output layer for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with Adam optimizer and a learning rate of 0.01\n",
        "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return test_accuracy\n",
        "\n",
        "max_features = 2000  # Make sure this matches the vocabulary size used in your text vectorization\n",
        "# Train the model and print the test accuracy\n",
        "test_accuracy = compile_and_train_lstm(X_train, y_train, X_val, y_val, input_shape=(1, max_features))\n",
        "print(f'Test accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "ctP_onnMVWsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. GRU"
      ],
      "metadata": {
        "id": "6B1v2V9FVWkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define and compile the GRU model in a function\n",
        "def compile_and_train_gru(X_train, y_train, X_val, y_val, input_shape, units=256, learning_rate=0.01, epochs=10, batch_size=256):\n",
        "    # Initialize the Sequential model\n",
        "    model = Sequential([\n",
        "        GRU(units, input_shape=input_shape, activation='tanh'),\n",
        "        Dense(2, activation='softmax')  # Output layer for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with Adam optimizer and a learning rate of 0.01\n",
        "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return test_accuracy\n",
        "\n",
        "max_features = 2000\n",
        "# Train the model and print the test accuracy\n",
        "test_accuracy = compile_and_train_gru(X_train, y_train, X_val, y_val, input_shape=(1, max_features))\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "vPB_yG20VWgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. BiLSTM"
      ],
      "metadata": {
        "id": "-74p0vEYVWcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define and compile the BiLSTM model in a function\n",
        "def compile_and_train_bilstm(X_train, y_train, X_val, y_val, input_shape, units=256, learning_rate=0.01, epochs=10, batch_size=256):\n",
        "    # Initialize the Sequential model\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(units, input_shape=input_shape, activation='tanh')),\n",
        "        Dense(2, activation='softmax')  # Output layer for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with Adam optimizer and a learning rate of 0.01\n",
        "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return test_accuracy\n",
        "\n",
        "max_features = 2000\n",
        "# Train the model and print the test accuracy\n",
        "test_accuracy = compile_and_train_bilstm(X_train, y_train, X_val, y_val, input_shape=(1, max_features))\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "s8sSEjo9uDSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Best accuracy\n",
        "LSTM model yields the best accuracy, which is 0.875."
      ],
      "metadata": {
        "id": "C3epbKG2uDEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2 - Training a simple chatbot using a seq-to-seq mode"
      ],
      "metadata": {
        "id": "zSI818L0SeHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Chatbot model"
      ],
      "metadata": {
        "id": "jOXpORTKSeFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JGQwTNZ2Fwfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d70170-69b9-4fc4-af30-7c5907355db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "metadata": {
        "id": "0ruzsZvVSeCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_name = \"movie-corpus\"\n",
        "drive_path = \"/content/drive/MyDrive/DS301\"\n",
        "corpus = os.path.join(drive_path, corpus_name)\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'r', encoding='utf-8') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus_path, \"utterances.jsonl\"))"
      ],
      "metadata": {
        "id": "sxfQ1NtzSd_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5f0b72-13b0-4abb-e4fa-1253871869b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let's go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you're gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I'm kidding.  You know how sometimes you just become this \\\"persona\\\"?  And you don't know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\"\", \"tag\": \"''\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n't\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\n",
            "\n",
            "{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits each line of the file to create lines and conversations\n",
        "def loadLinesAndConversations(fileName):\n",
        "    lines = {}\n",
        "    conversations = {}\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            lineJson = json.loads(line)\n",
        "            # Extract fields for line object\n",
        "            lineObj = {}\n",
        "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
        "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
        "            lineObj[\"text\"] = lineJson[\"text\"]\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "\n",
        "            # Extract fields for conversation object\n",
        "            if lineJson[\"conversation_id\"] not in conversations:\n",
        "                convObj = {}\n",
        "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
        "                convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
        "                convObj[\"lines\"] = [lineObj]\n",
        "            else:\n",
        "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
        "                convObj[\"lines\"].insert(0, lineObj)\n",
        "            conversations[convObj[\"conversationID\"]] = convObj\n",
        "\n",
        "    return lines, conversations\n",
        "\n",
        "\n",
        "# Extracts pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations.values():\n",
        "        # Iterate over all the lines of the conversation\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            # Filter wrong samples (if one of the lists is empty)\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "V5PNdZHfSd9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import csv\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/DS301/movie-corpus\"\n",
        "\n",
        "# Define path to new file\n",
        "datafile = os.path.join(drive_path, \"formatted_movie_lines.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "# Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# Initialize lines dict and conversations dict\n",
        "lines = {}\n",
        "conversations = {}\n",
        "\n",
        "print(\"\\nProcessing corpus into lines and conversations...\")\n",
        "lines, conversations = loadLinesAndConversations(os.path.join(drive_path, \"utterances.jsonl\"))\n",
        "\n",
        "# Write new csv file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "# Function to print a sample of lines\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'r', encoding='utf-8') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "# Print a sample of lines\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)\n"
      ],
      "metadata": {
        "id": "dlPNhVSxFi51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9446f1ba-9e81-4830-bfd6-56661b0f91a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing corpus into lines and conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "They do to!\tThey do not!\n",
            "\n",
            "She okay?\tI hope so.\n",
            "\n",
            "Wow\tLet's go.\n",
            "\n",
            "\"I'm kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don't know how to quit?\"\tNo\n",
            "\n",
            "No\tOkay -- you're gonna need to learn how to lie.\n",
            "\n",
            "I figured you'd get to the good stuff eventually.\tWhat good stuff?\n",
            "\n",
            "What good stuff?\t\"The \"\"real you\"\".\"\n",
            "\n",
            "\"The \"\"real you\"\".\"\tLike my fear of wearing pastels?\n",
            "\n",
            "do you listen to this crap?\tWhat crap?\n",
            "\n",
            "What crap?\tMe.  This endless ...blonde babble. I'm like, boring myself.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "UNK_token = 3  # Unknown word token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {\"UNK\": UNK_token}\n",
        "        self.word2count = {\"UNK\": 0}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
        "        self.num_words = 4  # Count SOS, EOS, PAD, UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Call this method to safely get the index of a word\n",
        "    def getIndex(self, word):\n",
        "        return self.word2index.get(word, UNK_token)\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count and k != \"UNK\":  # Don't trim the \"UNK\" token\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index) - 1,  # Exclude \"UNK\" token from the original count\n",
        "            float(len(keep_words)) / (len(self.word2index) - 1)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {\"UNK\": UNK_token}\n",
        "        self.word2count = {\"UNK\": 0}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
        "        self.num_words = 4  # Count default tokens plus \"UNK\"\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n"
      ],
      "metadata": {
        "id": "E_L04N70Fi0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# Read query/response pairs and return a voc object\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    # Read the file and split into lines\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "# Returns True if both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
        "def filterPair(p):\n",
        "    # Input sequences need to preserve the last word for EOS token\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# Filter pairs using the ``filterPair`` condition\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# Using the functions defined above, return a populated voc object and pairs list\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "\n",
        "# Load/Assemble voc and pairs\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "# Print some pairs to validate\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)"
      ],
      "metadata": {
        "id": "h_wk1DjPFiwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3346c8a-6ad0-401b-9c72-fcc23e016253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 64313 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 18083\n",
            "\n",
            "pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['the real you .', 'like my fear of wearing pastels ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n",
            "['but', 'you always been this selfish ?']\n",
            "['have fun tonight ?', 'tons']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used under the MIN_COUNT from the voc\n",
        "    voc.trim(MIN_COUNT)\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# Trim voc and pairs\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ],
      "metadata": {
        "id": "BQ3YmhyhFirr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89ccdca-ee94-4d1d-8258-8d993d69df9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 7833 / 18079 = 0.4333\n",
            "Trimmed from 64313 pairs to 53131, 0.8261 of total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "\n",
        "# Example for validation\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "metadata": {
        "id": "vcou4jAGFink",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b603dea-a42a-4272-efd8-c688725c649c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[ 38,  20,  20, 104,  20],\n",
            "        [285, 336,  18,  11,  11],\n",
            "        [ 20,  86,  23,   2,   2],\n",
            "        [  5, 258, 295,   0,   0],\n",
            "        [ 51,  26,  11,   0,   0],\n",
            "        [ 49,  11,   2,   0,   0],\n",
            "        [ 11,   2,   0,   0,   0],\n",
            "        [  2,   0,   0,   0,   0]])\n",
            "lengths: tensor([8, 7, 6, 3, 3])\n",
            "target_variable: tensor([[  17,   35,  278,  104,    5],\n",
            "        [  73,  141,   73,   20,   25],\n",
            "        [ 646,  269,   11,   11,  351],\n",
            "        [  63,  100,    2,    2,  353],\n",
            "        [ 202,  188,    0,    0,   73],\n",
            "        [5741,   11,    0,    0,   11],\n",
            "        [ 159,    2,    0,    0,    2],\n",
            "        [  25,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False,  True],\n",
            "        [ True,  True, False, False,  True],\n",
            "        [ True,  True, False, False,  True],\n",
            "        [ True, False, False, False, False],\n",
            "        [ True, False, False, False, False]])\n",
            "max_target_len: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size parameters are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "r4CdvWQEFijN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Luong attention layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "metadata": {
        "id": "z505-zi7Fie4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "ANdT4_W4JRYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "metadata": {
        "id": "0-xjQ4BuJRS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for RNN packing should always be on the CPU\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "metadata": {
        "id": "-qSc9vbhJRNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "metadata": {
        "id": "hCwII9-mJRJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ],
      "metadata": {
        "id": "xtwwCqNGJRF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "metadata": {
        "id": "7mHTVoaQJRBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#``attn_model = 'general'``\n",
        "#``attn_model = 'concat'``\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000"
      ],
      "metadata": {
        "id": "jIMledqHJQ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "                    '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "                    '{}_checkpoint.tar'.format(checkpoint_iter))"
      ],
      "metadata": {
        "id": "jjPokyJvSd6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/drive/MyDrive/DS301/movie-corpus/save_dir\"\n",
        "\n",
        "# Load model if a ``loadFilename`` is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "num_embeddings = voc.num_words # new code\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "metadata": {
        "id": "QajBkB1TJoAz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "4f7b5064-55c8-42e3-b5c8-57334dd65988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/save/cb_model/movie-corpus/2-2_500/4000_checkpoint.tar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-cfffced9a10e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloadFilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# If loading on same machine the model was trained on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadFilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# If loading a model trained on GPU to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/save/cb_model/movie-corpus/2-2_500/4000_checkpoint.tar'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have CUDA, configure CUDA to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ],
      "metadata": {
        "id": "6DNFOH3wJnpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set dropout layers to ``eval`` mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)"
      ],
      "metadata": {
        "id": "7NES4Ql8Jnk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin chatting (uncomment and run the following line to begin)\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "metadata": {
        "id": "mrDRzXaJJmfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At iteration 3998, the chatbot model is 100.0% complete, with the average loss being 2.4532. At iteration 4000, the average loss is 2.8663."
      ],
      "metadata": {
        "id": "Pn36XHxuJmkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. 3. W&B random search"
      ],
      "metadata": {
        "id": "Qb0hn3PGfDaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -Uq"
      ],
      "metadata": {
        "id": "iPDLctB0fDln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "CXD7TmrR7ppR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "gK5ITba0QL5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "      'name': 'loss',\n",
        "      'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': [0.0001, 0.00025, 0.0005, 0.001]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'sgd']\n",
        "        },\n",
        "        'clip': {\n",
        "            'values': [0, 25, 50, 100]\n",
        "        },\n",
        "        'teacher_forcing_ratio': {\n",
        "            'values': [0, 0.5, 1.0]\n",
        "        },\n",
        "        'decoder_learning_ratio': {\n",
        "            'values': [1.0, 3.0, 5.0, 10.0]\n",
        "        },\n",
        "        'fc_layer_size': {\n",
        "            'values': [128, 256, 512]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.3, 0.4, 0.5]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'distribution': 'q_log_uniform_values',\n",
        "            'q': 8,\n",
        "            'min': 32,\n",
        "            'max': 256\n",
        "        },\n",
        "        'epochs': {\n",
        "          'value': 1}\n",
        "    }\n",
        "}\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(sweep_config)"
      ],
      "metadata": {
        "id": "XAVtp35O7peJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Run the hyperparameter sweeps"
      ],
      "metadata": {
        "id": "19mB7OYd7pZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "\n",
        "        loader = build_dataset(config.batch_size)\n",
        "        network = build_network(config.fc_layer_size, config.dropout)\n",
        "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            avg_loss = train_epoch(network, loader, optimizer)\n",
        "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})"
      ],
      "metadata": {
        "id": "6XCzIoxI7pSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(batch_size):\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    # download MNIST training dataset\n",
        "    dataset = datasets.MNIST(\".\", train=True, download=True,\n",
        "                             transform=transform)\n",
        "    sub_dataset = torch.utils.data.Subset(\n",
        "        dataset, indices=range(0, len(dataset), 5))\n",
        "    loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size)\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "def build_network(fc_layer_size, dropout):\n",
        "    network = nn.Sequential(  # fully-connected, single hidden layer\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, fc_layer_size), nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(fc_layer_size, 10),\n",
        "        nn.LogSoftmax(dim=1))\n",
        "\n",
        "    return network.to(device)\n",
        "\n",
        "\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.parameters(),\n",
        "                               lr=learning_rate)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def train_epoch(network, loader, optimizer):\n",
        "    cumu_loss = 0\n",
        "    for _, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ➡ Forward pass\n",
        "        loss = F.nll_loss(network(data), target)\n",
        "        cumu_loss += loss.item()\n",
        "\n",
        "        # ⬅ Backward pass + weight update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\"batch loss\": loss.item()})\n",
        "\n",
        "    return cumu_loss / len(loader)"
      ],
      "metadata": {
        "id": "sfwJf84Z7pNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep_id = wandb.sweep(sweep_config)\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")\n",
        "# wandb.agent(sweep_id, function=train)\n",
        "wandb.agent(sweep_id, train, count=5)"
      ],
      "metadata": {
        "id": "TKh7mDPJ7pHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Best hyperparameters"
      ],
      "metadata": {
        "id": "qPkNcc2M7pCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api = wandb.Api()\n",
        "\n",
        "entity_name = 'huangytelina'\n",
        "project_name = 'pytorch-sweeps-demo'\n",
        "\n",
        "# Get the sweep\n",
        "sweep = api.sweep(f\"{entity_name}/{project_name}/{sweep_id}\")\n",
        "\n",
        "# Find the run with the lowest loss\n",
        "best_run = sorted(sweep.runs, key=lambda r: r.summary.get('loss', float('inf')))[0]\n",
        "best_run_id = best_run.id\n",
        "best_hyperparameters = best_run.config\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best Run ID: {best_run_id}\")\n",
        "print(\"Best Hyperparameters:\")\n",
        "for key, value in best_hyperparameters.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Download the model artifacts for the best run\n",
        "model_artifacts = best_run.logged_artifacts()\n",
        "model_path = 'path_to_save_model'\n",
        "for artifact in model_artifacts:\n",
        "    if artifact.type == 'model':\n",
        "        artifact.download(root=model_path)\n",
        "\n",
        "print(f\"Model downloaded to {model_path}\")"
      ],
      "metadata": {
        "id": "NSyT7G4m7o4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters such as learning rate, batch size, and the architecture depth are critical for model convergence. The learning rate controls the size of the steps taken during optimization and needs to be balanced to ensure that the model converges without overshooting the minimum loss. Batch size affects the stability and speed of the convergence, while the architecture depth can impact the model's ability to capture complex patterns in the data.\n",
        "\n",
        "The optimal hyperparameters, as highlighted by W&B feature importance, suggest a carefully tuned balance for convergence. A moderate learning rate (0.001) and batch size (80) ensure steady progress without overfitting, while dropout (0.3) aids generalization. The fully connected layer size (256) captures complex data patterns effectively. The chosen optimizer, SGD, is simple yet effective, indicating robustness for the dataset used. Teacher forcing (0) and a higher decoder learning ratio (3) hint at a specialized focus on prediction quality. Gradient clipping (25) provides stability by preventing exploding gradients, contributing to the model's successful training dynamics."
      ],
      "metadata": {
        "id": "qVzWjq1gfD0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3 - Attention in Transformer"
      ],
      "metadata": {
        "id": "XAL44WBaSppV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Encoder input vectors are transformed into Query (Q), Key (K), and Value (V) vectors through trained weight matrices, resulting in three distinct vectors per word.\n",
        "\n",
        "2. Self-attention calculates softmax scores by dot products of query and key vectors, scaled by the key vectors' dimension's square root, then normalized by softmax to yield probabilities summing to 1.\n",
        "\n",
        "3. In multi-headed attention, each head has its own set of weight matrices for the query, key, and value transformations. It uses 8 separate sets of weight matrices for transforming them. With 8 heads, each requiring 3 matrices (Q, K, and V), a total of 8*3=24 matrices are learned. Each head's matrices are sized 512x512, matching the input and output vector dimensions.\n",
        "\n",
        "4. To create a single matrix input for the feed-forward layer from multiple attention heads, the output vectors from all heads are concatenated. This concatenated output is then multiplied by a specifically trained weight matrix called the output linear layer, resulting in a single, unified vector per word. For 8 heads, each producing 512-dimensional vectors, the concatenated output is 4096-dimensional. It is then transformed by the output linear layer, which has a dimension of 8x512x512, to produce an output vector of size 512 for each word, compatible with the feed-forward layer’s expected input size."
      ],
      "metadata": {
        "id": "feyCMSniSpmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 4 - Using BERT for Question Answering"
      ],
      "metadata": {
        "id": "F-t2aRjMSpZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the transformers library that will be used for BERT models.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "160UGRcySpWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. BertTokenizer\n",
        "\n",
        "We will use the BertForQuestionAnswering model and the BertTokenizer as our tokenizer."
      ],
      "metadata": {
        "id": "j0jhTraNSpTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "#Get the pretrained 'bert-large-uncased-whole-word-masking-finetuned-squad' model from the BertForQuestionAnswering library\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "\"\"\"TO DO:\n",
        "# Similarly, get the tokenizer from pretrained 'bert-large-uncased-whole-word-masking-finetuned-squad' from the BertTokenizer library.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "metadata": {
        "id": "zLX0A4KgSpQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What was BERT trained on?\"\n",
        "\n",
        "paragraph = \"BERT stands for Bidirectional Encoder Representation of Transformer. I feel that its name itself is descriptive enough to get the gist. Still, to understand it better, it’s encoder part of the encoder-decoder transformer model, it’s also bidirectional in nature, which means that for any input it’s able to learn dependencies from both left and right of any word. It was trained on Wikipedia text and BooksCorpus and open-sourced back in 2018 by Google. You can find the official repository and paper at Github: BERT and BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. There are two models introduced in the paper. BERT base — 12 layers (transformer blocks), 110 million parameters. BERT Large — 24 layers, 340 million parameters. Later google also released Multi-lingual BERT to accelerate the research\""
      ],
      "metadata": {
        "id": "RwzA4qJeSpOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. encode_plus"
      ],
      "metadata": {
        "id": "NLp6wZMNwiiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode_plus(text=question, text_pair=paragraph, add_special_tokens=True)"
      ],
      "metadata": {
        "id": "a9OeLtz9wicK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Keys"
      ],
      "metadata": {
        "id": "VArcPPQLwiXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoding.keys())"
      ],
      "metadata": {
        "id": "Q5mu7mOxwiTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = encoding['input_ids']  # Token embeddings\n",
        "sentence_embedding = encoding['token_type_ids']  # Segment embeddings\n",
        "\n",
        "# We convert the input ids to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs)  # input tokens\n",
        "\n",
        "# The model returns the most probable start and end words scores.\n",
        "scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "DqRN3dfExLh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Index 5. Print output"
      ],
      "metadata": {
        "id": "t_Q9GJdjxLey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "start_index = torch.argmax(scores.start_logits)\n",
        "end_index = torch.argmax(scores.end_logits)\n",
        "\n",
        "if end_index >= start_index:\n",
        "    # Convert tokens to string from start to end index\n",
        "    answer = \" \".join(tokens[start_index:end_index + 1])\n",
        "    print(answer)\n",
        "else:\n",
        "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")"
      ],
      "metadata": {
        "id": "MFYXFRjjxLac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Unusual tokens\n",
        "The output looks unusual in the tokens \"##corp\" and \"##us\" because the tokens from the BERT tokenizer include special characters to indicate subword units. BERT uses a WordPiece tokenization method that breaks down words into smaller pieces so that the model can deal with a wide range of vocabulary with a limited set of learned embeddings."
      ],
      "metadata": {
        "id": "SZ63PykXzw6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 5 - Hyperparameter Optimization using H20"
      ],
      "metadata": {
        "id": "BSASWI8mSwcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Grid search"
      ],
      "metadata": {
        "id": "HvvSC-geSwZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o"
      ],
      "metadata": {
        "id": "rE3Ap4w7ZMmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c14113-4d89-4eb7-cb40-56ed98750e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting h2o\n",
            "  Downloading h2o-3.46.0.1-py2.py3-none-any.whl (265.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.6/265.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from h2o) (2.31.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from h2o) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->h2o) (2024.2.2)\n",
            "Installing collected packages: h2o\n",
            "Successfully installed h2o-3.46.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h2o\n",
        "from h2o.grid.grid_search import H2OGridSearch\n",
        "from h2o.estimators import H2ORandomForestEstimator\n",
        "\n",
        "# Initialize H2O\n",
        "h2o.init()\n",
        "\n",
        "airlines= h2o.import_file(\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip\")\n",
        "\n",
        "# Define the hyperparameters to search over\n",
        "hyper_parameters = {'ntrees': [10, 30, 50, 100], 'max_depth': [1, 2, 4, 6]}\n",
        "\n",
        "# Initialize the Grid Search with Random Forest estimator\n",
        "grid_search = H2OGridSearch(model=H2ORandomForestEstimator(seed=1234), hyper_params=hyper_parameters)\n",
        "\n",
        "# set the predictor names and the response column name\n",
        "predictors = [\"Origin\", \"Dest\", \"Year\", \"UniqueCarrier\",\n",
        "              \"DayOfWeek\", \"Month\", \"Distance\", \"FlightNum\"]\n",
        "response = \"IsDepDelayed\"\n",
        "\n",
        "# split into train and validation sets\n",
        "train, valid = airlines.split_frame(ratios = [.8], seed = 1234)\n",
        "\n",
        "# Train the models with the grid search\n",
        "grid_search.train(x=predictors, y=response, training_frame=train, validation_frame=valid)\n",
        "\n",
        "# Get the grid search results, sorted by accuracy in a decreasing order\n",
        "sorted_grid = grid_search.get_grid(sort_by='accuracy', decreasing=True)\n",
        "print(sorted_grid)"
      ],
      "metadata": {
        "id": "o5PMWJRASwS1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "outputId": "2877e6ef-29c4-41a1-e6c7-43881c97305b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
            "Attempting to start a local H2O server...\n",
            "  Java Version: openjdk version \"11.0.22\" 2024-01-16; OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1); OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n",
            "  Starting server from /usr/local/lib/python3.10/dist-packages/h2o/backend/bin/h2o.jar\n",
            "  Ice root: /tmp/tmpv9wz9yu3\n",
            "  JVM stdout: /tmp/tmpv9wz9yu3/h2o_unknownUser_started_from_python.out\n",
            "  JVM stderr: /tmp/tmpv9wz9yu3/h2o_unknownUser_started_from_python.err\n",
            "  Server is running at http://127.0.0.1:54321\n",
            "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         02 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.1\n",
              "H2O_cluster_version_age:    26 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_99ggro\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    12.75 Gb\n",
              "H2O_cluster_total_cores:    8\n",
              "H2O_cluster_allowed_cores:  8\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://127.0.0.1:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-1.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-1 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-1 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-1 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-1 .h2o-table th,\n",
              "#h2o-table-1 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>02 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.1</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>26 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_99ggro</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>12.75 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>8</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>8</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://127.0.0.1:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
            "drf Grid Build progress: |███████████████████████████████████████████████████████| (done) 100%\n",
            "Hyper-Parameter Search Summary: ordered by decreasing accuracy\n",
            "    max_depth    ntrees    model_ids                                                     accuracy\n",
            "--  -----------  --------  ------------------------------------------------------------  ----------\n",
            "    6            30        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_8   0.663\n",
            "    6            50        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_12  0.662771\n",
            "    6            100       Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_16  0.661969\n",
            "    6            10        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_4   0.660823\n",
            "    4            50        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_11  0.65051\n",
            "    4            100       Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_15  0.648677\n",
            "    4            30        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_7   0.648562\n",
            "    4            10        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_3   0.643864\n",
            "    2            50        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_10  0.632405\n",
            "    2            30        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_6   0.63103\n",
            "    2            100       Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_14  0.630228\n",
            "    1            50        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_9   0.6143\n",
            "    2            10        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_2   0.614186\n",
            "    1            100       Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_13  0.612925\n",
            "    1            30        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_5   0.602613\n",
            "    1            10        Grid_DRF_py_2_sid_8a8d_model_python_1712601730295_1_model_1   0.601238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the best model and evaluate the model’s performance on a test set\n",
        "best_model = sorted_grid.models[0]\n",
        "best_model_perf = best_model.model_performance(valid)\n",
        "print(\"The best model is model_8 with max_depth 6 and 30 trees\")\n",
        "print(f\"AUC Score on validation set: {best_model_perf.auc()}\")"
      ],
      "metadata": {
        "id": "ghdXTiUmSwP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7823dc1d-2fa6-480a-a9bb-2879c201f416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best model is model_8 with max_depth 6 and 30 trees\n",
            "AUC Score on validation set: 0.7166542527113227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Randomized grid search"
      ],
      "metadata": {
        "id": "epeG2t4tWTBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify search criteria for randomized grid search\n",
        "search_criteria = {\"strategy\": \"RandomDiscrete\", \"max_models\": 10, 'seed': 42}\n",
        "\n",
        "train, valid = airlines.split_frame(ratios=[.8], seed=1234)\n",
        "\n",
        "# Initialize the Randomized Grid Search with Random Forest estimator\n",
        "grid_search = H2OGridSearch(model=H2ORandomForestEstimator(seed=1234),\n",
        "                            hyper_params=hyper_parameters,\n",
        "                            search_criteria=search_criteria)\n",
        "\n",
        "# Train the models with the randomized grid search\n",
        "grid_search.train(x=predictors, y=response, training_frame=train)\n",
        "\n",
        "# Get the grid search results, sorted by accuracy in a decreasing order\n",
        "sorted_grid = grid_search.get_grid(sort_by='accuracy', decreasing=True)\n",
        "print(sorted_grid)"
      ],
      "metadata": {
        "id": "gZdUZA0wWS9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc366a69-c565-4bdd-bdb3-708c6dd9d387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drf Grid Build progress: |███████████████████████████████████████████████████████| (done) 100%\n",
            "Hyper-Parameter Search Summary: ordered by decreasing accuracy\n",
            "    max_depth    ntrees    model_ids                                                        accuracy\n",
            "--  -----------  --------  ---------------------------------------------------------------  ----------\n",
            "    6            100       Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_6   0.671016\n",
            "    6            30        Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_4   0.667697\n",
            "    4            100       Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_8   0.659357\n",
            "    6            10        Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_7   0.658113\n",
            "    4            30        Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_10  0.653258\n",
            "    4            10        Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_1   0.638525\n",
            "    2            100       Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_2   0.636209\n",
            "    2            50        Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_3   0.635727\n",
            "    2            30        Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_5   0.630989\n",
            "    1            10        Grid_DRF_py_5_sid_8a8d_model_python_1712601730295_1115_model_9   0.592073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the best model and evaluate the model’s performance on a test set\n",
        "best_model = sorted_grid.models[0]\n",
        "best_model_perf = best_model.model_performance(valid)\n",
        "print(\"The best model is model_6 with max_depth 6 and 100 trees\")\n",
        "print(f\"AUC Score on validation set: {best_model_perf.auc()}\")"
      ],
      "metadata": {
        "id": "T7jOzfccWS0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42181c97-57e4-426f-f900-356090251994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best model is model_6 with max_depth 6 and 100 trees\n",
            "AUC Score on validation set: 0.7188120050200918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. H2O AutoML"
      ],
      "metadata": {
        "id": "q_uyuAypWSvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat the preparation process\n",
        "# Initialize H2O\n",
        "h2o.init()\n",
        "\n",
        "airlines= h2o.import_file(\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip\")\n",
        "\n",
        "# Define the hyperparameters to search over\n",
        "hyper_parameters = {'ntrees': [10, 30, 50, 100], 'max_depth': [1, 2, 4, 6]}\n",
        "\n",
        "# Initialize the Grid Search with Random Forest estimator\n",
        "grid_search = H2OGridSearch(model=H2ORandomForestEstimator(seed=1234), hyper_params=hyper_parameters)\n",
        "\n",
        "# set the predictor names and the response column name\n",
        "predictors = [\"Origin\", \"Dest\", \"Year\", \"UniqueCarrier\",\n",
        "              \"DayOfWeek\", \"Month\", \"Distance\", \"FlightNum\"]\n",
        "response = \"IsDepDelayed\"\n",
        "\n",
        "# split into train and validation sets\n",
        "train, valid = airlines.split_frame(ratios = [.8], seed = 1234)"
      ],
      "metadata": {
        "id": "W4JrKsiU2Tyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "240cc85b-396d-4605-c7b1-5dbf3ab0179c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "--------------------------  -----------------------------------------------------------------------------------------\n",
              "H2O_cluster_uptime:         1 min 25 secs\n",
              "H2O_cluster_timezone:       Etc/UTC\n",
              "H2O_data_parsing_timezone:  UTC\n",
              "H2O_cluster_version:        3.46.0.1\n",
              "H2O_cluster_version_age:    26 days\n",
              "H2O_cluster_name:           H2O_from_python_unknownUser_99ggro\n",
              "H2O_cluster_total_nodes:    1\n",
              "H2O_cluster_free_memory:    12.74 Gb\n",
              "H2O_cluster_total_cores:    8\n",
              "H2O_cluster_allowed_cores:  8\n",
              "H2O_cluster_status:         locked, healthy\n",
              "H2O_connection_url:         http://localhost:54321\n",
              "H2O_connection_proxy:       {\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}\n",
              "H2O_internal_security:      False\n",
              "Python_version:             3.10.12 final\n",
              "--------------------------  -----------------------------------------------------------------------------------------"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "#h2o-table-2.h2o-container {\n",
              "  overflow-x: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table {\n",
              "  /* width: 100%; */\n",
              "  margin-top: 1em;\n",
              "  margin-bottom: 1em;\n",
              "}\n",
              "#h2o-table-2 .h2o-table caption {\n",
              "  white-space: nowrap;\n",
              "  caption-side: top;\n",
              "  text-align: left;\n",
              "  /* margin-left: 1em; */\n",
              "  margin: 0;\n",
              "  font-size: larger;\n",
              "}\n",
              "#h2o-table-2 .h2o-table thead {\n",
              "  white-space: nowrap; \n",
              "  position: sticky;\n",
              "  top: 0;\n",
              "  box-shadow: 0 -1px inset;\n",
              "}\n",
              "#h2o-table-2 .h2o-table tbody {\n",
              "  overflow: auto;\n",
              "}\n",
              "#h2o-table-2 .h2o-table th,\n",
              "#h2o-table-2 .h2o-table td {\n",
              "  text-align: right;\n",
              "  /* border: 1px solid; */\n",
              "}\n",
              "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
              "  /* background: #F5F5F5 */\n",
              "}\n",
              "\n",
              "</style>      \n",
              "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
              "  <table class=\"h2o-table\">\n",
              "    <caption></caption>\n",
              "    <thead></thead>\n",
              "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
              "<td>1 min 25 secs</td></tr>\n",
              "<tr><td>H2O_cluster_timezone:</td>\n",
              "<td>Etc/UTC</td></tr>\n",
              "<tr><td>H2O_data_parsing_timezone:</td>\n",
              "<td>UTC</td></tr>\n",
              "<tr><td>H2O_cluster_version:</td>\n",
              "<td>3.46.0.1</td></tr>\n",
              "<tr><td>H2O_cluster_version_age:</td>\n",
              "<td>26 days</td></tr>\n",
              "<tr><td>H2O_cluster_name:</td>\n",
              "<td>H2O_from_python_unknownUser_99ggro</td></tr>\n",
              "<tr><td>H2O_cluster_total_nodes:</td>\n",
              "<td>1</td></tr>\n",
              "<tr><td>H2O_cluster_free_memory:</td>\n",
              "<td>12.74 Gb</td></tr>\n",
              "<tr><td>H2O_cluster_total_cores:</td>\n",
              "<td>8</td></tr>\n",
              "<tr><td>H2O_cluster_allowed_cores:</td>\n",
              "<td>8</td></tr>\n",
              "<tr><td>H2O_cluster_status:</td>\n",
              "<td>locked, healthy</td></tr>\n",
              "<tr><td>H2O_connection_url:</td>\n",
              "<td>http://localhost:54321</td></tr>\n",
              "<tr><td>H2O_connection_proxy:</td>\n",
              "<td>{\"http\": null, \"https\": null, \"colab_language_server\": \"/usr/colab/bin/language_service\"}</td></tr>\n",
              "<tr><td>H2O_internal_security:</td>\n",
              "<td>False</td></tr>\n",
              "<tr><td>Python_version:</td>\n",
              "<td>3.10.12 final</td></tr></tbody>\n",
              "  </table>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "# Run AutoML for 20 base models\n",
        "aml = H2OAutoML(max_models=10, seed=1, max_runtime_secs=1000) # Run for at most 100 secs to save running time\n",
        "aml.train(x=predictors, y=response, training_frame=train)\n",
        "\n",
        "# View the AutoML Leaderboard\n",
        "lb = aml.leaderboard\n",
        "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)"
      ],
      "metadata": {
        "id": "767a7UpuWSlk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "1a8be996-136f-469c-c836-970edb77c07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML progress: |███████████████████████████████████████████████████████████████| (done) 100%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_id                                                      auc    logloss     aucpr    mean_per_class_error      rmse       mse\n",
              "-------------------------------------------------------  --------  ---------  --------  ----------------------  --------  --------\n",
              "StackedEnsemble_AllModels_1_AutoML_1_20240408_184340     0.751711   0.589492  0.764765                0.355465  0.449521  0.202069\n",
              "StackedEnsemble_BestOfFamily_1_AutoML_1_20240408_184340  0.750975   0.590115  0.764132                0.339008  0.449827  0.202345\n",
              "GBM_1_AutoML_1_20240408_184340                           0.746962   0.593601  0.759656                0.353689  0.451444  0.203801\n",
              "GBM_4_AutoML_1_20240408_184340                           0.745632   0.595072  0.756256                0.353863  0.452035  0.204336\n",
              "XRT_1_AutoML_1_20240408_184340                           0.743993   0.597298  0.754893                0.36765   0.453099  0.205298\n",
              "XGBoost_2_AutoML_1_20240408_184340                       0.743973   0.596507  0.757167                0.364245  0.452756  0.204988\n",
              "XGBoost_1_AutoML_1_20240408_184340                       0.743243   0.598759  0.756401                0.35556   0.453596  0.205749\n",
              "GBM_3_AutoML_1_20240408_184340                           0.742187   0.598218  0.751502                0.364613  0.453418  0.205588\n",
              "GBM_2_AutoML_1_20240408_184340                           0.741271   0.599021  0.751022                0.351863  0.453806  0.20594\n",
              "XGBoost_3_AutoML_1_20240408_184340                       0.736816   0.602151  0.750317                0.361275  0.455632  0.207601\n",
              "DRF_1_AutoML_1_20240408_184340                           0.730975   0.621951  0.742943                0.370249  0.4616    0.213075\n",
              "GLM_1_AutoML_1_20240408_184340                           0.689424   0.636708  0.702118                0.444746  0.472162  0.222937\n",
              "[12 rows x 7 columns]\n"
            ],
            "text/html": [
              "<table class='dataframe'>\n",
              "<thead>\n",
              "<tr><th>model_id                                               </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>StackedEnsemble_AllModels_1_AutoML_1_20240408_184340   </td><td style=\"text-align: right;\">0.751711</td><td style=\"text-align: right;\"> 0.589492</td><td style=\"text-align: right;\">0.764765</td><td style=\"text-align: right;\">              0.355465</td><td style=\"text-align: right;\">0.449521</td><td style=\"text-align: right;\">0.202069</td></tr>\n",
              "<tr><td>StackedEnsemble_BestOfFamily_1_AutoML_1_20240408_184340</td><td style=\"text-align: right;\">0.750975</td><td style=\"text-align: right;\"> 0.590115</td><td style=\"text-align: right;\">0.764132</td><td style=\"text-align: right;\">              0.339008</td><td style=\"text-align: right;\">0.449827</td><td style=\"text-align: right;\">0.202345</td></tr>\n",
              "<tr><td>GBM_1_AutoML_1_20240408_184340                         </td><td style=\"text-align: right;\">0.746962</td><td style=\"text-align: right;\"> 0.593601</td><td style=\"text-align: right;\">0.759656</td><td style=\"text-align: right;\">              0.353689</td><td style=\"text-align: right;\">0.451444</td><td style=\"text-align: right;\">0.203801</td></tr>\n",
              "<tr><td>GBM_4_AutoML_1_20240408_184340                         </td><td style=\"text-align: right;\">0.745632</td><td style=\"text-align: right;\"> 0.595072</td><td style=\"text-align: right;\">0.756256</td><td style=\"text-align: right;\">              0.353863</td><td style=\"text-align: right;\">0.452035</td><td style=\"text-align: right;\">0.204336</td></tr>\n",
              "<tr><td>XRT_1_AutoML_1_20240408_184340                         </td><td style=\"text-align: right;\">0.743993</td><td style=\"text-align: right;\"> 0.597298</td><td style=\"text-align: right;\">0.754893</td><td style=\"text-align: right;\">              0.36765 </td><td style=\"text-align: right;\">0.453099</td><td style=\"text-align: right;\">0.205298</td></tr>\n",
              "<tr><td>XGBoost_2_AutoML_1_20240408_184340                     </td><td style=\"text-align: right;\">0.743973</td><td style=\"text-align: right;\"> 0.596507</td><td style=\"text-align: right;\">0.757167</td><td style=\"text-align: right;\">              0.364245</td><td style=\"text-align: right;\">0.452756</td><td style=\"text-align: right;\">0.204988</td></tr>\n",
              "<tr><td>XGBoost_1_AutoML_1_20240408_184340                     </td><td style=\"text-align: right;\">0.743243</td><td style=\"text-align: right;\"> 0.598759</td><td style=\"text-align: right;\">0.756401</td><td style=\"text-align: right;\">              0.35556 </td><td style=\"text-align: right;\">0.453596</td><td style=\"text-align: right;\">0.205749</td></tr>\n",
              "<tr><td>GBM_3_AutoML_1_20240408_184340                         </td><td style=\"text-align: right;\">0.742187</td><td style=\"text-align: right;\"> 0.598218</td><td style=\"text-align: right;\">0.751502</td><td style=\"text-align: right;\">              0.364613</td><td style=\"text-align: right;\">0.453418</td><td style=\"text-align: right;\">0.205588</td></tr>\n",
              "<tr><td>GBM_2_AutoML_1_20240408_184340                         </td><td style=\"text-align: right;\">0.741271</td><td style=\"text-align: right;\"> 0.599021</td><td style=\"text-align: right;\">0.751022</td><td style=\"text-align: right;\">              0.351863</td><td style=\"text-align: right;\">0.453806</td><td style=\"text-align: right;\">0.20594 </td></tr>\n",
              "<tr><td>XGBoost_3_AutoML_1_20240408_184340                     </td><td style=\"text-align: right;\">0.736816</td><td style=\"text-align: right;\"> 0.602151</td><td style=\"text-align: right;\">0.750317</td><td style=\"text-align: right;\">              0.361275</td><td style=\"text-align: right;\">0.455632</td><td style=\"text-align: right;\">0.207601</td></tr>\n",
              "<tr><td>DRF_1_AutoML_1_20240408_184340                         </td><td style=\"text-align: right;\">0.730975</td><td style=\"text-align: right;\"> 0.621951</td><td style=\"text-align: right;\">0.742943</td><td style=\"text-align: right;\">              0.370249</td><td style=\"text-align: right;\">0.4616  </td><td style=\"text-align: right;\">0.213075</td></tr>\n",
              "<tr><td>GLM_1_AutoML_1_20240408_184340                         </td><td style=\"text-align: right;\">0.689424</td><td style=\"text-align: right;\"> 0.636708</td><td style=\"text-align: right;\">0.702118</td><td style=\"text-align: right;\">              0.444746</td><td style=\"text-align: right;\">0.472162</td><td style=\"text-align: right;\">0.222937</td></tr>\n",
              "</tbody>\n",
              "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[12 rows x 7 columns]</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the best performing model and print its parameters\n",
        "print(aml.leader)\n",
        "print(\"Parameters:\", aml.leader.params)"
      ],
      "metadata": {
        "id": "j5rW_qwtWShQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0013c06c-0017-48ec-def6-72171e3afc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Details\n",
            "=============\n",
            "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
            "Model Key: StackedEnsemble_AllModels_1_AutoML_1_20240408_184340\n",
            "\n",
            "\n",
            "Model Summary for Stacked Ensemble: \n",
            "key                                   value\n",
            "------------------------------------  ----------------\n",
            "Stacking strategy                     cross_validation\n",
            "Number of base models (used / total)  7/10\n",
            "# GBM base models (used / total)      2/4\n",
            "# XGBoost base models (used / total)  3/3\n",
            "# DRF base models (used / total)      2/2\n",
            "# GLM base models (used / total)      0/1\n",
            "Metalearner algorithm                 GLM\n",
            "Metalearner fold assignment scheme    Random\n",
            "Metalearner nfolds                    5\n",
            "Metalearner fold_column\n",
            "Custom metalearner hyperparameters    None\n",
            "\n",
            "ModelMetricsBinomialGLM: stackedensemble\n",
            "** Reported on train data. **\n",
            "\n",
            "MSE: 0.17746762744143907\n",
            "RMSE: 0.42126906774820183\n",
            "LogLoss: 0.5319711860278803\n",
            "AUC: 0.8185480248240496\n",
            "AUCPR: 0.8360351136154335\n",
            "Gini: 0.6370960496480993\n",
            "Null degrees of freedom: 10066\n",
            "Residual degrees of freedom: 10059\n",
            "Null deviance: 13937.138934382425\n",
            "Residual deviance: 10710.707859485341\n",
            "AIC: 10726.707859485341\n",
            "\n",
            "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4208109862281492\n",
            "       NO    YES    Error    Rate\n",
            "-----  ----  -----  -------  ----------------\n",
            "NO     2826  1989   0.4131   (1989.0/4815.0)\n",
            "YES    790   4462   0.1504   (790.0/5252.0)\n",
            "Total  3616  6451   0.2761   (2779.0/10067.0)\n",
            "\n",
            "Maximum Metrics: Maximum metrics at their respective thresholds\n",
            "metric                       threshold    value     idx\n",
            "---------------------------  -----------  --------  -----\n",
            "max f1                       0.420811     0.76254   255\n",
            "max f2                       0.277541     0.860293  325\n",
            "max f0point5                 0.597348     0.760599  162\n",
            "max accuracy                 0.516444     0.738353  204\n",
            "max precision                0.971411     1         0\n",
            "max recall                   0.119507     1         391\n",
            "max specificity              0.971411     1         0\n",
            "max absolute_mcc             0.516444     0.477204  204\n",
            "max min_per_class_accuracy   0.508897     0.736862  208\n",
            "max mean_per_class_accuracy  0.516444     0.738827  204\n",
            "max tns                      0.971411     4815      0\n",
            "max fns                      0.971411     5245      0\n",
            "max fps                      0.086507     4815      399\n",
            "max tps                      0.119507     5252      391\n",
            "max tnr                      0.971411     1         0\n",
            "max fnr                      0.971411     0.998667  0\n",
            "max fpr                      0.086507     1         399\n",
            "max tpr                      0.119507     1         391\n",
            "\n",
            "Gains/Lift Table: Avg response rate: 52.17 %, avg score: 52.29 %\n",
            "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
            "-------  --------------------------  -----------------  --------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
            "1        0.0100328                   0.927855           1.91679   1.91679            1                0.948728  1                           0.948728            0.0192308       0.0192308                  91.6794   91.6794            0.0192308\n",
            "2        0.0200656                   0.908481           1.91679   1.91679            1                0.916724  1                           0.932726            0.0192308       0.0384615                  91.6794   91.6794            0.0384615\n",
            "3        0.029999                    0.895003           1.87846   1.9041             0.98             0.901497  0.993377                    0.922385            0.0186596       0.0571211                  87.8458   90.41              0.0567057\n",
            "4        0.0400318                   0.880044           1.85986   1.89301            0.970297         0.887201  0.987593                    0.913567            0.0186596       0.0757807                  85.9859   89.3012            0.0747422\n",
            "5        0.0500646                   0.866687           1.85986   1.88637            0.970297         0.8735    0.984127                    0.905538            0.0186596       0.0944402                  85.9859   88.6368            0.0927787\n",
            "6        0.10003                     0.817081           1.7758    1.83114            0.926441         0.842893  0.955313                    0.874247            0.0887281       0.183168                   77.5797   83.1137            0.173823\n",
            "7        0.149995                    0.776825           1.71864   1.79366            0.89662          0.796588  0.935762                    0.848378            0.085872        0.26904                    71.8636   79.3662            0.248895\n",
            "8        0.200159                    0.739103           1.60555   1.74652            0.837624         0.758398  0.911166                    0.825827            0.0805407       0.349581                   60.5552   74.6518            0.312406\n",
            "9        0.29999                     0.664538           1.43044   1.64133            0.746269         0.702181  0.856291                    0.78468             0.142803        0.492384                   43.0443   64.1334            0.402249\n",
            "10       0.40002                     0.588061           1.24297   1.54172            0.648461         0.625482  0.804321                    0.744871            0.124334        0.616717                   24.2965   54.1717            0.453062\n",
            "11       0.50005                     0.515339           1.11734   1.45682            0.58292          0.551117  0.760032                    0.706112            0.111767        0.728484                   11.7336   45.6824            0.477602\n",
            "12       0.59998                     0.446246           0.882182  1.36111            0.460239         0.480904  0.710099                    0.668602            0.0881569       0.816641                   -11.7818  36.1114            0.452986\n",
            "13       0.70001                     0.37985            0.704284  1.26725            0.367428         0.412942  0.661132                    0.632069            0.0704494       0.887091                   -29.5716  26.7254            0.39114\n",
            "14       0.79994                     0.309838           0.581135  1.18154            0.303181         0.345436  0.616416                    0.596262            0.0580731       0.945164                   -41.8865  18.1543            0.303627\n",
            "15       0.89997                     0.23713            0.37308   1.09168            0.194638         0.273615  0.569536                    0.560401            0.0373191       0.982483                   -62.692   9.16838            0.172514\n",
            "16       1                           0.0854058          0.175119  1                  0.0913605        0.185882  0.521705                    0.522938            0.0175171       1                          -82.4881  0                  0\n",
            "\n",
            "ModelMetricsBinomialGLM: stackedensemble\n",
            "** Reported on cross-validation data. **\n",
            "\n",
            "MSE: 0.20206907969773288\n",
            "RMSE: 0.44952094467080494\n",
            "LogLoss: 0.5894916401670246\n",
            "AUC: 0.7517113314357008\n",
            "AUCPR: 0.7647650980576783\n",
            "Gini: 0.5034226628714016\n",
            "Null degrees of freedom: 35250\n",
            "Residual degrees of freedom: 35243\n",
            "Null deviance: 48785.012143164495\n",
            "Residual deviance: 41560.33961505557\n",
            "AIC: 41576.33961505557\n",
            "\n",
            "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.35988866668692265\n",
            "       NO    YES    Error    Rate\n",
            "-----  ----  -----  -------  -----------------\n",
            "NO     6949  9817   0.5855   (9817.0/16766.0)\n",
            "YES    2318  16167  0.1254   (2318.0/18485.0)\n",
            "Total  9267  25984  0.3442   (12135.0/35251.0)\n",
            "\n",
            "Maximum Metrics: Maximum metrics at their respective thresholds\n",
            "metric                       threshold    value     idx\n",
            "---------------------------  -----------  --------  -----\n",
            "max f1                       0.359889     0.727113  280\n",
            "max f2                       0.173149     0.848117  370\n",
            "max f0point5                 0.566314     0.70777   177\n",
            "max accuracy                 0.483599     0.68781   217\n",
            "max precision                0.97108      1         0\n",
            "max recall                   0.0713605    1         399\n",
            "max specificity              0.97108      1         0\n",
            "max absolute_mcc             0.515912     0.37416   202\n",
            "max min_per_class_accuracy   0.51355      0.685962  203\n",
            "max mean_per_class_accuracy  0.515912     0.687293  202\n",
            "max tns                      0.97108      16766     0\n",
            "max fns                      0.97108      18466     0\n",
            "max fps                      0.0713605    16766     399\n",
            "max tps                      0.0713605    18485     399\n",
            "max tnr                      0.97108      1         0\n",
            "max fnr                      0.97108      0.998972  0\n",
            "max fpr                      0.0713605    1         399\n",
            "max tpr                      0.0713605    1         399\n",
            "\n",
            "Gains/Lift Table: Avg response rate: 52.44 %, avg score: 52.43 %\n",
            "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
            "-------  --------------------------  -----------------  --------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
            "1        0.0100139                   0.924521           1.80976   1.80976            0.949008         0.943037  0.949008                    0.943037            0.0181228       0.0181228                  80.9765   80.9765            0.0170492\n",
            "2        0.0200278                   0.905865           1.71252   1.76114            0.898017         0.91457   0.923513                    0.928804            0.017149        0.0352718                  71.2524   76.1144            0.032051\n",
            "3        0.0300133                   0.891037           1.71739   1.74659            0.900568         0.898305  0.915879                    0.918657            0.017149        0.0524209                  71.7389   74.6586            0.0471125\n",
            "4        0.0400272                   0.87893            1.69091   1.73266            0.886686         0.884985  0.908575                    0.910233            0.0169326       0.0693535                  69.0914   73.2659            0.0616594\n",
            "5        0.0500128                   0.867963           1.69572   1.72528            0.889205         0.873572  0.904708                    0.902913            0.0169326       0.0862862                  69.5718   72.5283            0.0762659\n",
            "6        0.100026                    0.817622           1.64524   1.68526            0.862734         0.842563  0.883721                    0.872738            0.0822829       0.168569                   64.5239   68.5261            0.144115\n",
            "7        0.15001                     0.774412           1.51305   1.62788            0.793417         0.795369  0.853631                    0.846958            0.0756289       0.244198                   51.305    62.7879            0.198033\n",
            "8        0.200023                    0.735428           1.46784   1.58786            0.769711         0.754456  0.832648                    0.823829            0.0734109       0.317609                   46.7843   58.7864            0.247228\n",
            "9        0.30002                     0.661943           1.33572   1.50382            0.700426         0.699239  0.788578                    0.782303            0.133568        0.451177                   33.5715   50.3823            0.317811\n",
            "10       0.400017                    0.589691           1.20966   1.43029            0.634326         0.625641  0.750018                    0.74314             0.120963        0.57214                    20.9664   43.0288            0.361893\n",
            "11       0.500014                    0.518604           1.05494   1.35522            0.553191         0.554064  0.710655                    0.705327            0.105491        0.677631                   5.49393   35.5223            0.373443\n",
            "12       0.600011                    0.450266           0.932134  1.28471            0.488794         0.483588  0.67368                     0.668372            0.0932107       0.770841                   -6.78664  28.4711            0.359175\n",
            "13       0.700009                    0.384401           0.76713   1.21077            0.40227          0.41746   0.634908                    0.632529            0.0767108       0.847552                   -23.287   21.0774            0.310215\n",
            "14       0.800006                    0.314571           0.657308  1.14159            0.344681         0.349776  0.598631                    0.597186            0.065729        0.913281                   -34.2692  14.1593            0.238165\n",
            "15       0.900003                    0.241022           0.519896  1.07252            0.272624         0.278211  0.562409                    0.561746            0.0519881       0.965269                   -48.0104  7.25179            0.137224\n",
            "16       1                           0.0662999          0.347318  1                  0.182128         0.18777   0.524382                    0.524349            0.0347309       1                          -65.2682  0                  0\n",
            "\n",
            "Cross-Validation Metrics Summary: \n",
            "                      mean        sd            cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
            "--------------------  ----------  ------------  ------------  ------------  ------------  ------------  ------------\n",
            "accuracy              0.66165817  0.01003698    0.6604982     0.65549695    0.64948165    0.66784954    0.6749644\n",
            "aic                   8328.295    80.49933      8251.452      8307.53       8462.641      8289.177      8330.675\n",
            "auc                   0.75174266  0.0031944418  0.7558943     0.7491507     0.75005674    0.7544528     0.7491588\n",
            "err                   0.33834183  0.01003698    0.33950177    0.34450307    0.35051835    0.33215046    0.32503557\n",
            "err_count             2385.6      81.65966      2385.0        2416.0        2502.0        2340.0        2285.0\n",
            "f0point5              0.665644    0.008598204   0.6630912     0.6628679     0.6550643     0.6689603     0.67823625\n",
            "f1                    0.7280795   0.0029405367  0.73241335    0.7280504     0.72517574    0.7291667     0.7255914\n",
            "f2                    0.8037609   0.014591705   0.8179221     0.8074503     0.8120941     0.80128205    0.78005576\n",
            "lift_top_group        1.8106078   0.06628049    1.8283068     1.7020941     1.8820484     1.8104844     1.8301054\n",
            "loglikelihood         0.0         0.0           0.0           0.0           0.0           0.0           0.0\n",
            "---                   ---         ---           ---           ---           ---           ---           ---\n",
            "mean_per_class_error  0.34877926  0.012390736   0.3508416     0.3580426     0.36225075    0.34057146    0.33218983\n",
            "mse                   0.20207317  0.0010904565  0.2006708     0.2027673     0.2028265     0.20111808    0.20298322\n",
            "null_deviance         9757.002    71.00812      9722.749      9698.697      9878.985      9754.191      9730.389\n",
            "pr_auc                0.76475483  0.0021740063  0.7676002     0.7651847     0.7634417     0.7656413     0.76190627\n",
            "precision             0.62973595  0.01310359    0.623734      0.6255319     0.61539894    0.634058      0.64995694\n",
            "r2                    0.1897534   0.004625193   0.19548737    0.18608062    0.18682727    0.19407597    0.18629578\n",
            "recall                0.86386526  0.026427248   0.8869565     0.8707593     0.88262033    0.85784316    0.821147\n",
            "residual_deviance     8312.295    80.49933      8235.452      8291.53       8446.641      8273.177      8314.675\n",
            "rmse                  0.4495242   0.0012135688  0.44796294    0.4502969     0.45036262    0.44846192    0.45053658\n",
            "specificity           0.43857622  0.049343318   0.41136023    0.4131555     0.39287817    0.46101394    0.5144733\n",
            "[24 rows x 8 columns]\n",
            "\n",
            "Parameters: {'model_id': {'default': None, 'actual': {'__meta': {'schema_version': 3, 'schema_name': 'ModelKeyV3', 'schema_type': 'Key<Model>'}, 'name': 'StackedEnsemble_AllModels_1_AutoML_1_20240408_184340', 'type': 'Key<Model>', 'URL': '/3/Models/StackedEnsemble_AllModels_1_AutoML_1_20240408_184340'}, 'input': None}, 'training_frame': {'default': None, 'actual': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'AutoML_1_20240408_184340_training_py_8_sid_9368', 'type': 'Key<Frame>', 'URL': '/3/Frames/AutoML_1_20240408_184340_training_py_8_sid_9368'}, 'input': {'__meta': {'schema_version': 3, 'schema_name': 'FrameKeyV3', 'schema_type': 'Key<Frame>'}, 'name': 'AutoML_1_20240408_184340_training_py_8_sid_9368', 'type': 'Key<Frame>', 'URL': '/3/Frames/AutoML_1_20240408_184340_training_py_8_sid_9368'}}, 'response_column': {'default': None, 'actual': {'__meta': {'schema_version': 3, 'schema_name': 'ColSpecifierV3', 'schema_type': 'VecSpecifier'}, 'column_name': 'IsDepDelayed', 'is_member_of_frames': None}, 'input': {'__meta': {'schema_version': 3, 'schema_name': 'ColSpecifierV3', 'schema_type': 'VecSpecifier'}, 'column_name': 'IsDepDelayed', 'is_member_of_frames': None}}, 'validation_frame': {'default': None, 'actual': None, 'input': None}, 'blending_frame': {'default': None, 'actual': None, 'input': None}, 'base_models': {'default': [], 'actual': [{'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GBM_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GBM_4_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'XRT_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'XGBoost_2_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'XGBoost_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GBM_3_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GBM_2_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'XGBoost_3_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'DRF_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GLM_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}], 'input': [{'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GBM_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GBM_4_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'XRT_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'XGBoost_2_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'XGBoost_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GBM_3_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GBM_2_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'XGBoost_3_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'DRF_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}, {'__meta': {'schema_version': 3, 'schema_name': 'KeyV3', 'schema_type': 'Key<Keyed>'}, 'name': 'GLM_1_AutoML_1_20240408_184340', 'type': 'Key<Keyed>', 'URL': None}]}, 'metalearner_algorithm': {'default': 'AUTO', 'actual': 'glm', 'input': 'AUTO'}, 'metalearner_nfolds': {'default': 0, 'actual': 5, 'input': 5}, 'metalearner_fold_assignment': {'default': None, 'actual': None, 'input': None}, 'metalearner_fold_column': {'default': None, 'actual': None, 'input': None}, 'metalearner_params': {'default': '', 'actual': '', 'input': ''}, 'metalearner_transform': {'default': 'NONE', 'actual': 'Logit', 'input': 'Logit'}, 'max_runtime_secs': {'default': 0.0, 'actual': 0.0, 'input': 0.0}, 'weights_column': {'default': None, 'actual': None, 'input': None}, 'offset_column': {'default': None, 'actual': None, 'input': None}, 'custom_metric_func': {'default': None, 'actual': None, 'input': None}, 'seed': {'default': -1, 'actual': 12, 'input': 12}, 'score_training_samples': {'default': 10000, 'actual': 10000, 'input': 10000}, 'keep_levelone_frame': {'default': False, 'actual': True, 'input': True}, 'export_checkpoints_dir': {'default': None, 'actual': None, 'input': None}, 'auc_type': {'default': 'AUTO', 'actual': 'AUTO', 'input': 'AUTO'}, 'gainslift_bins': {'default': -1, 'actual': -1, 'input': -1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the AUC score of the best model for the test set\n",
        "leader_perf = aml.leader.model_performance(valid)\n",
        "print(f\"AUC Score of the best model on the test set: {leader_perf.auc()}\")"
      ],
      "metadata": {
        "id": "cCRWYuZQ2kCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6839d66e-f254-4f33-ad08-ffb5901d0989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score of the best model on the test set: 0.7543269369063046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the best XGBoost model using logloss\n",
        "\n",
        "# Get the full leaderboard sorted by log loss\n",
        "import pandas as pd\n",
        "# Convert leaderboard to a Pandas DataFrame\n",
        "lb_df = lb.as_data_frame()\n",
        "# Sort the DataFrame by log loss\n",
        "lb_df_sorted = lb_df.sort_values(by='logloss', ascending=True)\n",
        "# Display the sorted DataFrame\n",
        "print(lb_df_sorted)"
      ],
      "metadata": {
        "id": "_3YiMYBxEJj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b291a56-7937-45f0-9cb8-8ce8d4fcc886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             model_id       auc   logloss  \\\n",
            "0   StackedEnsemble_AllModels_1_AutoML_1_20240408_...  0.751711  0.589492   \n",
            "1   StackedEnsemble_BestOfFamily_1_AutoML_1_202404...  0.750975  0.590115   \n",
            "2                      GBM_1_AutoML_1_20240408_184340  0.746962  0.593601   \n",
            "3                      GBM_4_AutoML_1_20240408_184340  0.745632  0.595072   \n",
            "5                  XGBoost_2_AutoML_1_20240408_184340  0.743973  0.596507   \n",
            "4                      XRT_1_AutoML_1_20240408_184340  0.743993  0.597298   \n",
            "7                      GBM_3_AutoML_1_20240408_184340  0.742187  0.598218   \n",
            "6                  XGBoost_1_AutoML_1_20240408_184340  0.743243  0.598759   \n",
            "8                      GBM_2_AutoML_1_20240408_184340  0.741271  0.599021   \n",
            "9                  XGBoost_3_AutoML_1_20240408_184340  0.736816  0.602151   \n",
            "10                     DRF_1_AutoML_1_20240408_184340  0.730975  0.621951   \n",
            "11                     GLM_1_AutoML_1_20240408_184340  0.689424  0.636708   \n",
            "\n",
            "       aucpr  mean_per_class_error      rmse       mse  \n",
            "0   0.764765              0.355465  0.449521  0.202069  \n",
            "1   0.764132              0.339008  0.449827  0.202345  \n",
            "2   0.759656              0.353689  0.451444  0.203801  \n",
            "3   0.756256              0.353863  0.452035  0.204336  \n",
            "5   0.757167              0.364245  0.452756  0.204988  \n",
            "4   0.754893              0.367650  0.453099  0.205298  \n",
            "7   0.751502              0.364613  0.453418  0.205588  \n",
            "6   0.756401              0.355560  0.453596  0.205749  \n",
            "8   0.751022              0.351863  0.453806  0.205940  \n",
            "9   0.750317              0.361275  0.455632  0.207601  \n",
            "10  0.742943              0.370249  0.461600  0.213075  \n",
            "11  0.702118              0.444746  0.472162  0.222937  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/h2o/frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install datatable (for Python 3.9 or lower), or polars and pyarrow (for Python 3.10 or above) and activate it using:\n",
            "\n",
            "with h2o.utils.threading.local_context(polars_enabled=True, datatable_enabled=True):\n",
            "    pandas_df = h2o_df.as_data_frame()\n",
            "\n",
            "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By log loss, the best XGBoost model is XGBoost_2_AutoML_3_20240402_181949, with AUC 0.743973 and log loss 0.596507."
      ],
      "metadata": {
        "id": "KpDkiQMUFF7h"
      }
    }
  ]
}